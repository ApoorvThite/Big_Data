{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS/CMPSC 410 Spring 2025\n",
    "# Instructor: Professor John Yen\n",
    "# TAs: Jin Peng and Jingxi Zhu\n",
    "\n",
    "## Lab 8 Decision Tree Learning Using ML Pipeline, Visualization, and Hyperparameter Tuning\n",
    "\n",
    "## The goals of this lab are for you to be able to\n",
    "- Understand the function of the different steps/stages involved in Spark ML pipeline\n",
    "- Be able to construct a decision tree using Spark ML machine learning module\n",
    "- Be able to generate a visualization of Decision Trees\n",
    "- Be able to perform automated hyper-parameter tuning for Decision Trees \n",
    "\n",
    "## The data set used in this lab is a Breast Cancer diagnosis dataset.\n",
    "\n",
    "## Submit the following items for Lab 8 (DT)\n",
    "- Completed Jupyter Notebook of Lab 8 (in HTML format)\n",
    "- A visualization of the decision tree generated in Part 5.\n",
    "- The output file that contains the best DT hyperparameters for Part 6.\n",
    "- A visualization of the best decision tree generated in Part 6.\n",
    "- The output file that contains the best DT hyperparameters for Part 7.\n",
    "- a visualization of the best decision tree generated in Part 7.\n",
    "\n",
    "## Total Number of Exercises: 8\n",
    "- Exercise 1: 5 points\n",
    "- Exercise 2: 5 points\n",
    "- Exercise 3: 10 points  \n",
    "- Exercise 4: 10 points \n",
    "- Exercise 5: 20 points\n",
    "- Exercise 6: 10 points\n",
    "- Exercise 7: 30 points\n",
    "- Exercise 8: 10 points\n",
    "## Total Points: 100 points\n",
    "\n",
    "# Due: midnight, March 23, 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and set up the Python files for this Lab based on the instructions in \"SpecialInstructionsLab 8\" in Canvas (under Module Topic and Lab 8)\n",
    "1. Create a \"Lab8DT\" directory in the work directory of your work directory.\n",
    "2. Create a subdirectory under \"Lab8DT\" called \"decision_tree_plot\" (named the directory EXACTLY this way).\n",
    "3. Upload the following three files in Module 8 from Canvas to the decision_tree_plot subdirectory\n",
    "- decision_tree_parser.py\n",
    "- decision_tree_plot.py\n",
    "- tree_template.jinjia2\n",
    "4. After you have completed the steps above, upload the Jupyter Notebook for Lab 8 (Lab8_DT_F24.ipynb) to the Lab8DT directory.\n",
    "5. Upload the data file breast-cancer-wisconsin.data.txt from module 8 in Canvas to the \"Lab8DT\" directory.\n",
    "6. Open the Jupyter Notebook for Lab 8 and follow the instructions to complete the lab.\n",
    "\n",
    "# Follow the instructions below and execute the PySpark code cell by cell below. Make modifications as required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notice that we use PySpark SQL module to import SparkSession because ML works with SparkSession\n",
    "## Notice also the different methods imported from ML and three submodules of ML: classification, feature, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField, StructType, StringType, LongType, IntegerType, FloatType\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler, IndexToString\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The following two lines import relevant functions from the two python files you uploaded into the decision_tree_plot subdirectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decision_tree_plot.decision_tree_parser import decision_tree_parse\n",
    "from decision_tree_plot.decision_tree_plot import plot_trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This lab runs Spark in the local mode because the size of data is small. \n",
    "## When you need to develop a Decision Tree-based Predictive Model for a large dataset, you will need to debug the code in local mode using a small sampled data.  After running in local mode successfully, you will need to convert it for cluster mode for running using the large dataset, like previous labs.\n",
    "### Notice we are creating a SparkSession, not a SparkContext, when we use ML pipeline.\n",
    "### The \"getOrCreate()\" method means we can re-evaluate this without a need to \"stop the current SparkSession\" first (unlike SparkContext)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss=SparkSession.builder.master(\"local\").appName(\"lab 8 DT\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.sparkContext.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: (5 points) Enter your name below:\n",
    "- My Name: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As we have seen in previous labs, we can define the schema for reading the input data into a PySpark DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: (5 points) Complete the following path with the path for your home directory.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_schema = StructType([ StructField(\"id\", IntegerType(), False ), \\\n",
    "                        StructField(\"clump_thickness\", IntegerType(), False), \\\n",
    "                        StructField(\"unif_cell_size\", IntegerType(), False ), \\\n",
    "                        StructField(\"unif_cell_shape\", IntegerType(), False ), \\\n",
    "                        StructField(\"marg_adhesion\", IntegerType(), False), \\\n",
    "                        StructField(\"single_epith_cell_size\", IntegerType(), False), \\\n",
    "                        StructField(\"bare_nuclei\", IntegerType(), False),\\\n",
    "                        StructField(\"bland_chrom\", IntegerType(), False), \\\n",
    "                        StructField(\"norm_nucleoli\", IntegerType(), False), \\\n",
    "                        StructField(\"mitoses\", IntegerType(), False), \\\n",
    "                        StructField(\"class\", StringType(), False) \\\n",
    "                           ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ss.read.csv(\"/storage/home/juy1/work/Lab8DT/breast-cancer-wisconsin.data.txt\", schema=bc_schema, header=True, inferSchema=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 Feature Transformation Using DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "class_count = data.groupBy(col(\"class\")).count()\n",
    "class_count.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting and Filtering Rows with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.filter(col(\"bare_nuclei\").isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = data.filter(col(\"bare_nuclei\").isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "class_count2 = data2.groupBy(col(\"class\")).count()\n",
    "class_count2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We will use data2 (rather than data) in all of the remaining lab, because it does not contain missing/null values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 Feature Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StringIndex\n",
    "- Transforms a column of string to a new column of index (type double).\n",
    "- The feature transformation involves three steps:\n",
    "-- Step 1: Create a \"transformer\" \n",
    "-- Step 2: Use the data (which contains all possible values of the string column) to create a mapping (of these strings into an integer/index)\n",
    "-- Step 3: Use the mapping to generate the new column's value (i.e., trasformed index) for each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelIndexer= StringIndexer(inputCol=\"class\", outputCol=\"indexedLabel\").fit(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_data = labelIndexer.transform(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_features = ['clump_thickness', 'unif_cell_size', 'unif_cell_shape', 'marg_adhesion', \\\n",
    "                  'single_epith_cell_size', 'bare_nuclei', 'bland_chrom', 'norm_nucleoli', 'mitoses']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=input_features, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_data = assembler.transform(transformed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We will use `vectorized_data` for splitting labelled data into training data and testing data. This way, we have access to all the original features, which we will need for generating decision tree visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized2_data = vectorized_data.select(\"features\",'indexedLabel')\n",
    "vectorized2_data.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 Decision Tree Learning and Evaluation (1 hyperparameter setting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## randomSplit is a method for DataFrame that split data in the DataFrame into two subsets, one for training, the other for testing, using a number as the seed for random number generator.\n",
    "## If you want to generate a different split, you can use a different seed (preferably a prime number)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData, testData= vectorized_data.randomSplit([0.75, 0.25], seed=1237)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt=DecisionTreeClassifier(featuresCol=\"features\", labelCol=\"indexedLabel\", maxDepth=6, minInstancesPerNode=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_model = dt.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prediction = dt_model.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prediction.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To compare the actual labels and predicted labels more easily, we can select the following columns.\n",
    "## The `probability` column records the probability for the row to be in the \"zero/benign class\" and the probability to be in the \"one/malignant class\".\n",
    "## The `prediction` column records the predicted label for each row, which is the class with the higher probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prediction.select(\"features\",\"class\",\"indexedLabel\", \"probability\", \"prediction\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelIndexer.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelConverter=IndexToString(inputCol=\"prediction\", outputCol=\"predictedClass\", labels=labelIndexer.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2_prediction = labelConverter.transform(test_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2_prediction.select(\"features\",\"class\",\"indexedLabel\",\"prediction\",\"predictedClass\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = evaluator.evaluate(test_prediction)\n",
    "print(\"f1 score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4 DT Learning Using ML Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: (10 points) In the code cell below, fill in a value for maxDepth (recommend: 2 to 5) and a value of minInstancesPerNode (recommend: 1 to 5). Run the entire sequence of code below to generate a decision tree (using pipeline) and compute f1 measure of the testing data.\n",
    "- After you run the code successfully, record the f1 measure and your choice of max_depth and minInstancesPerNode below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer for Exercise 3: \n",
    "- The f1 measure of testing data for max_detph ??? and minInstancesPerNode ??? = ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData, testData= data2.randomSplit([0.8, 0.2], seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelIndexer= StringIndexer(inputCol=\"class\", outputCol=\"indexedLabel\").fit(data2)\n",
    "assembler = VectorAssembler( inputCols=input_features, outputCol=\"features\")\n",
    "dt=DecisionTreeClassifier(labelCol=\"indexedLabel\", featuresCol=\"features\", maxDepth=??, minInstancesPerNode=??)\n",
    "predictionConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedClass\", labels=labelIndexer.labels)\n",
    "pipeline = Pipeline(stages=[labelIndexer, assembler, dt, predictionConverter])\n",
    "model = pipeline.fit(trainingData)\n",
    "predictions = model.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.select(\"class\",\"indexedLabel\",\"features\",\"prediction\",\"predictedClass\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = evaluator.evaluate(predictions)\n",
    "print(\"f1 score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5 Decision Tree Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stages[2] of the pipeline is \"dt\" (DecisionTreeClassifier). \n",
    "## model is a DataFrame representing a trained pipeline.\n",
    "## model.stages[2] gives us the Decision Tree model learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTmodel = model.stages[2]\n",
    "print(DTmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: (10 points) \n",
    "- Complete the code below to generate a visualization of the decision tree.\n",
    "- Fill in your PSU ID in the path for ``model_path`` and ``output_path``\n",
    "- Save the visualization of the tree in a file that replaces ??? with your first initial and last name.  For example, I would name the file as \"DTree_jyen_Part5.html\".\n",
    "- Download the HTML file of the tree and submit it as a part of Lab8 assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path=\"/storage/home/???/work/Lab8DT/DTmodel_vis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tree=decision_tree_parse(DTmodel, ss, model_path)\n",
    "column = dict([(str(idx), i) for idx, i in enumerate(input_features)])\n",
    "plot_trees(tree, column = column, output_path = '/storage/home/???/work/Lab8DT/DTree_??_Part5.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 6 Automated Hyperparameter Tuning for Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: (20 points)  \n",
    "- Complete the code below to perform hyper parameter tuning of Decision Tree (for two parameters: max_depth and minInstancesPerNode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_features = ['clump_thickness', 'unif_cell_size', 'unif_cell_shape', 'marg_adhesion', \\\n",
    "                  'single_epith_cell_size', 'bare_nuclei', 'bland_chrom', 'norm_nucleoli', 'mitoses']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData, testingData= data2.randomSplit([0.8, 0.2], seed=12347)\n",
    "model_path=\"/storage/home/???/work/Lab8DT/DTmodel_vis_Part6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Initialize a Pandas DataFrame to store evaluation results of all combination of hyper-parameter settings\n",
    "hyperparams_eval_df = pd.DataFrame( columns = ['max_depth', 'minInstancesPerNode', 'training f1', \\\n",
    "                                               'testing f1', 'Best Model'] )\n",
    "# initialize index to the hyperparam_eval_df to 0\n",
    "index =0 \n",
    "# initialize lowest_error\n",
    "highest_testing_f1 = 0\n",
    "# Set up the possible hyperparameter values to be evaluated\n",
    "max_depth_list = [2, 3]\n",
    "minInstancesPerNode_list = [2, 3]\n",
    "labelIndexer = StringIndexer(inputCol=\"class\", outputCol=\"indexedLabel\").fit(data2)\n",
    "assembler = VectorAssembler( inputCols=input_features, outputCol=\"features\")\n",
    "labelConverter = IndexToString(inputCol = \"prediction\", outputCol=\"predictedClass\", labels=labelIndexer.labels)\n",
    "trainingData.persist()\n",
    "testingData.persist()\n",
    "for max_depth in max_depth_list:\n",
    "    for minInstPN in minInstancesPerNode_list:\n",
    "        seed = 37\n",
    "        # Construct a DT model using a set of hyper-parameter values and training data\n",
    "        dt= DecisionTreeClassifier(labelCol=\"indexedLabel\", featuresCol=\"features\", maxDepth= ???, \\\n",
    "                                   minInstancesPerNode= ???)\n",
    "        pipeline = Pipeline(stages=[???, ???, dt, ???])\n",
    "        model = pipeline.fit(trainingData)\n",
    "        training_predictions = model.transform(trainingData)\n",
    "        testing_predictions = model.transform(testingData)\n",
    "        evaluator = MulticlassClassificationEvaluator(labelCol=\"indexedLabel\", predictionCol=\"prediction\", \\\n",
    "                                                      metricName=\"f1\")\n",
    "        training_f1 = evaluator.evaluate(training_predictions)\n",
    "        testing_f1 = evaluator.evaluate(testing_predictions)\n",
    "        # We use 0 as default value of the 'Best Model' column in the Pandas DataFrame.\n",
    "        # The best model will have a value 1000\n",
    "        hyperparams_eval_df.loc[index] = [ max_depth, minInstPN, training_f1, testing_f1, 0]  \n",
    "        index = index +1\n",
    "        if testing_f1 > ??? :\n",
    "            best_max_depth = max_depth\n",
    "            best_minInstPN = minInstPN\n",
    "            best_index = index -1\n",
    "            best_parameters_training_f1 = training_f1\n",
    "            best_DTmodel= model.stages[??]\n",
    "            best_tree = decision_tree_parse(best_DTmodel, ss, model_path)\n",
    "            column = dict( [ (str(idx), i) for idx, i in enumerate(input_features) ])           \n",
    "            highest_testing_f1 = testing_f1\n",
    "print('The best max_depth is ', best_max_depth, ', best minInstancesPerNode = ', \\\n",
    "      best_minInstPN, ', testing f1 = ', highest_testing_f1) \n",
    "column = dict([(str(idx), i) for idx, i in enumerate(input_features)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_path=\"/storage/home/???/work/Lab8DT/BestDTmodel_???_Part6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_tree=decision_tree_parse(best_DTmodel, ss, best_model_path)\n",
    "column = dict([(str(idx), i) for idx, i in enumerate(input_features)])\n",
    "plot_trees(best_tree, column = column, output_path = '/storage/home/???/work/Lab8DT/BestDTtree_???_tuned_Part6.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the Testing RMS in the DataFrame\n",
    "hyperparams_eval_df.loc[best_index]=[best_max_depth, best_minInstPN, best_parameters_training_f1, highest_testing_f1, 1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6 (10 points)\n",
    "### Complete the path below to save the result of your hyperparameter tuning in a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"/storage/home/???/work/Lab8DT/Lab8DT_HyperparamsTuningResult_Table.csv\"\n",
    "hyperparams_eval_df.to_csv(output_path)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 7 A Revised Hyper-parameter Tuning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 7 (20 points) Copy the code for Part 6 to the the code cells below and modify the range of hyper-parameter tuning into the following:\n",
    "- max_depth: 2 to 11\n",
    "- minInstancesPerNode: 2 to 10\n",
    "## Modify the file names of your Decision Tree visualization files (.html) so that you do not accidentally destroy the visualiztion generated for Exercise 5 (Part 6).\n",
    "## Modify the file names of your output files so that you can compare the results of Part 7 (both decision tree and best hyper parameters) with those of Part 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code cell for Part 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code cell for Part 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code cell for Part 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code cell for Part 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 8 (10 points) Compare (a) the hyper-parameters and (b) the decision trees generated by Part 6 and Part 7. Discuss the results in the Markdown cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer for Exercise 8: \n",
    "- (a)\n",
    "- (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
